{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Roboflow-EfficientNet-Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7Evp4GOUt7Z"
      },
      "source": [
        "# How to Train EfficientNet to recognize custom classifications\n",
        "\n",
        "\n",
        "Many, many props to DLogogy for publishing the orginal version of this tutorial. [How to do Transfer learning with EfficientNet](https://www.dlology.com/blog/transfer-learning-with-efficientnet/)\n",
        "\n",
        "We use the same training infrastructure here with preprocessing, augmentations, and data import from Roboflow.\n",
        "\n",
        "You only need to change one line of code for your dataset import to train this classification model.\n",
        "\n",
        "You can scale the EfficientNet model up including:\n",
        "\n",
        "* EfficientNet-B0\n",
        "* EfficientNet-B1\n",
        "* EfficientNet-B2\n",
        "* EfficientNet-B3\n",
        "\n",
        "Consider reading the [Original Paper](https://arxiv.org/pdf/1905.11946.pdf)\n",
        "\n",
        "We recommend having the [Train EfficientNet Blog Post](https://blog.roboflow.ai/how-to-train-efficientnet/) up side by side to this notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYbI0TGpL6Jd"
      },
      "source": [
        "## Import EfficientNet Dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4wFQJkBiFWE"
      },
      "source": [
        "# Downgrade pillow to avoid `UserWarning: Possibly corrupt EXIF data.`\n",
        "# !pip install pillow==4.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "482XTFalCfB-"
      },
      "source": [
        "# %tensorflow_version 1.x\n",
        "#in case your keras version has bumped ahead you may want to try reverting to 2.3.1\n",
        "# !pip install q keras==2.3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras==2.8"
      ],
      "metadata": {
        "id": "yBtDIBQ-rC28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MlLZ1siDhV9"
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7SaH6Z1jqS_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmzQmZdKDfSF"
      },
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import sys\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9S4UyubKPUc"
      },
      "source": [
        "## Clone **EfficinetNet** repo\n",
        "\n",
        "Credit to [DLogogy](https://www.dlology.com/blog/transfer-learning-with-efficientnet/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnyJvYF_yXLo"
      },
      "source": [
        "import os\n",
        "%cd /content\n",
        "if not os.path.isdir(\"efficientnet_keras_transfer_learning\"):\n",
        "  !git clone https://github.com/Tony607/efficientnet_keras_transfer_learning\n",
        "%cd efficientnet_keras_transfer_learning/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO7kxxRaMC2q"
      },
      "source": [
        "\n",
        "\n",
        "## Import efficientnet and Choose EfficientNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq00KoYUzOSc"
      },
      "source": [
        "# Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "# Higher the number, the more complex the model is.\n",
        "\n",
        "#Choose\n",
        "#EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\n",
        "from efficientnet import EfficientNetB0 as Net\n",
        "from efficientnet import center_crop_and_resize, preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeETTKsszRj0"
      },
      "source": [
        "# loading pretrained conv base model\n",
        "\n",
        "#define input height and width\n",
        "width = 150\n",
        "height = 150\n",
        "input_shape = (height, width, 3)\n",
        "\n",
        "\n",
        "conv_base = Net(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WONAK_7CKY6z"
      },
      "source": [
        "## Import Custom Dataset From Roboflow\n",
        "\n",
        "To get your data into Roboflow, follow the [Getting Started Guide](https://blog.roboflow.ai/getting-started-with-roboflow/).\n",
        "\n",
        "After loading your dataset in by dragging and dropping, you will only need to change one line of code with the curl link to train your own custom classification model.\n",
        "\n",
        "Before upload, make sure that your data is in the structure\n",
        "\n",
        "folder\n",
        "\n",
        "---class1\n",
        "    \n",
        "------images\n",
        "  \n",
        "---class2\n",
        "\n",
        "------images\n",
        "  \n",
        "  ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hogQDF7KYsmC"
      },
      "source": [
        "!curl -L \"[YOUR LINK HERE]\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "EZQgptFzsk2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "ZLh2flbNsoJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip rock.zip"
      ],
      "metadata": {
        "id": "VLQdW1ncsq0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_GD-iEkPVin"
      },
      "source": [
        "train_dir = '/content/train/'\n",
        "valid_dir = '/content/valid/'\n",
        "test_dir = '/content/test/'\n",
        "batch_size = 4\n",
        "\n",
        "\n",
        "import os\n",
        "import random\n",
        "def delete_all_but(split_path, number):\n",
        "  images = []\n",
        "  for path, subdirs, files in os.walk(split_path):\n",
        "      for name in files:\n",
        "          images.append(os.path.join(path, name))\n",
        "  if len(images) > number:\n",
        "    keep = random.sample(images, number)\n",
        "    for img in images:\n",
        "      if img not in keep:\n",
        "        os.remove(img)\n",
        "  return None\n",
        "\n",
        "\n",
        "delete_all_but(train_dir, 25)\n",
        "delete_all_but(valid_dir, 25)\n",
        "delete_all_but(test_dir, 25)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9MM2WA49zyj"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255)\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory\n",
        "        train_dir,\n",
        "        # All images will be resized to target height and width.\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        valid_dir,\n",
        "        target_size=(height, width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "train_generator.class_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMq6-unWWWnu"
      },
      "source": [
        "# Set up EfficientNet Training Job"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp4TOw5rJs0M"
      },
      "source": [
        "import os, os.path\n",
        "epochs = 500\n",
        "NUM_TRAIN = sum([len(files) for r, d, files in os.walk(train_dir)])\n",
        "NUM_TEST = sum([len(files) for r, d, files in os.walk(valid_dir)])\n",
        "dropout_rate = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgjoz26l-HfD"
      },
      "source": [
        "num_classes = len(os.listdir(train_dir))\n",
        "print('building netowrk for ' + str(num_classes) + ' classes')\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(3, activation='softmax', name=\"fc_out\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iAX7AegDHUS"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N-F9Z-uDJAi"
      },
      "source": [
        "print('This is the number of trainable layers '\n",
        "      'before freezing the conv base:', len(model.trainable_weights))\n",
        "\n",
        "conv_base.trainable = False\n",
        "\n",
        "print('This is the number of trainable layers '\n",
        "      'after freezing the conv base:', len(model.trainable_weights))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly4ikbcSWeqV"
      },
      "source": [
        "# Run EfficientNet Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJHDUrjw2wRG"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOTgQ2x9WiCN"
      },
      "source": [
        "# Examine EfficientNet Training Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i90iKXuL3CHT"
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gRv7SwYqT9f"
      },
      "source": [
        "## Fine Tuning EfficientNet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKI8AJxQVB1Q"
      },
      "source": [
        "# multiply_16\n",
        "# set 'multiply_16' and following layers trainable\n",
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "    if layer.name == 'multiply_16':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMjt8tCcDOoC"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch= NUM_TRAIN // batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps= NUM_TEST // batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65AKWOGiHB5y"
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcYikQOyWsQf"
      },
      "source": [
        "# Save EfficientNet Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvVt5TwoEDz6"
      },
      "source": [
        "os.makedirs(\"./models\", exist_ok=True)\n",
        "model.save('./models/efficientNet.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bigZr9kjL2sY"
      },
      "source": [
        "# Use EfficientNet Trained Model for Inference\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWTv_-7CHqZB"
      },
      "source": [
        "import random\n",
        "test_dir = '/content/efficientnet_keras_transfer_learning/test/'\n",
        "test_imgs = []\n",
        "for path, subdirs, files in os.walk(test_dir):\n",
        "    for name in files:\n",
        "        test_imgs.append(os.path.join(path, name))\n",
        "random_test_image = random.choice(test_imgs)\n",
        "random_test_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VFoNC0TLzaf"
      },
      "source": [
        "Image(filename=random_test_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLDHANW_HPVv"
      },
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "#recover class names from the train dataset generator\n",
        "class_lookup = dict((y,x) for x,y in train_generator.class_indices.items())\n",
        "\n",
        "def predict_image(img_path, class_lookup):\n",
        "    # Read the image and resize it\n",
        "    img = image.load_img(img_path, target_size=(height, width))\n",
        "    # Convert it to a Numpy array with target shape.\n",
        "    x = image.img_to_array(img)\n",
        "    # Reshape\n",
        "    x = x.reshape((1,) + x.shape)\n",
        "    x /= 255.\n",
        "    result = model.predict([x])[0][0]\n",
        "    result_verbose = model.predict([x])\n",
        "    if result > 0.5:\n",
        "        animal = \"cat\"\n",
        "    else:\n",
        "        animal = \"dog\"\n",
        "        result = 1 - result\n",
        "    print(result_verbose)\n",
        "    predicted_class = class_lookup[np.argmax(result_verbose, axis=1)[0]]\n",
        "    predicted_probability = result_verbose[0][np.argmax(result_verbose, axis=1)[0]]\n",
        "\n",
        "    return predicted_class ,predicted_probability, result_verbose\n",
        "\n",
        "print(predict_image(random_test_image, class_lookup))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogB16XX3eXaN"
      },
      "source": [
        "## Download Trained EfficientNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_3Zek4eeSXD"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('./models/efficientNet.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1rvrVx3CVN0"
      },
      "source": [
        "model.input_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9O9mCbbVPWk"
      },
      "source": [
        "## Load Trained EfficientNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkBE5AZdwLki"
      },
      "source": [
        "from efficientnet.layers import Swish, DropConnect\n",
        "from efficientnet.model import ConvKernalInitializer\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "\n",
        "get_custom_objects().update({\n",
        "    'ConvKernalInitializer': ConvKernalInitializer,\n",
        "    'Swish': Swish,\n",
        "    'DropConnect':DropConnect\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pURT5B8uVFgq"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model(\"./models/efficientNet.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}